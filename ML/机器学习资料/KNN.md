##### KNN算法介绍

​      又名K最邻近分类算法，是数据挖掘分类技术中最简单的方法之一，所谓K最近邻，就是K个最近的邻居的意思，说的是每个样本都可以用它最接近的K个邻居来代表。

- 核心：

  如果一个样本在特征空间中的K个最相邻的样本中的大多数都属于某一个类别，则该样本也属于该类别，并且具有这个类别上的样本特性。

- 解释：

  假设有如下图例：

  ![0_1272437957U444](/Users/chenchen42/Documents/学习总结/机器学习/0_1272437957U444.gif)

  ​

  我们的目标是判断中心的绿色圆球是属于哪一类？

  KNN算法的核心处理过程如下（K代表选择多少个离待分类的元素的样本来分析）：

  假设K=2：由于离绿色圆球最近的两个图形为红色的三角形，红色的三角形的比率为1，所以绿色的圆球属于红色的三角形类别。

  假设K=5：由于离绿色圆球最近的五个图形为（3个蓝色方块，2个红色三角形），蓝色方块的比率为$\frac{3}{5}$,红色三角形出现的比例为$\frac{2}{5}$，因为蓝色方块出现的比率比较大，故绿色圆球属于蓝色方块类别。

  **TIPS:从上述的分析可以看出，K的选择可以决定最终的分类结果，也决定了分类的准确性**

##### 算法描述

```shell
令K为最邻近的数量，D是训练样例的集合。
for 每个测试样例z(x`,y`) do
	计算z和每个样例(x,y)∈D之间的距离d(x`,x)
    选择离z最近的k个训练样例的集合Dz∈D
    选择集合Dz出现频率最大的类别
end for
```

- 优点：
  - 简单，易于理解，易于实现，无需估计参数，无需训练。
  - 适合对稀有事件进行分类。
  - 特别适合于多分类问题，对象具有多个标签。
- 缺点：
  - 样本需要全部读入到内存中，样本很大时，需要大量内存空间用来存储样本。
  - 计算量较大。

##### 应用

假设用户有两个标签，分别是A和B标签，lable为对某一产品是否感兴趣，如果感兴趣就是1，不感兴趣就是0。数据集为：

```Shell
[
 [0.6970,0.4600,1],
 [0.7740,0.3760,1],
 [0.6340,0.2640,1],
 [0.6080,0.3180,1],
 [0.5560,0.2150,1],
 [0.4030,0.2370,1],
 [0.4810,0.1490,1],
 [0.4370,0.2110,1],
 [0.6660,0.0910,0],
 [0.2430,0.2670,0],
 [0.2450,0.0570,0],
 [0.3430,0.0990,0],
 [0.6390,0.1610,0],
 [0.6570,0.1980,0],
 [0.3600,0.3700,0],
 [0.5930,0.0420,0],
 [0.7190,0.1030,0]
]
```

测试数据集为（即预测具有下面标签的用户是否对该产品感兴趣）：

```shell
[
 [0.3590,0.1880],
 [0.3390,0.2410],
 [0.2820,0.2570],
 [0.7480,0.2320],
 [0.7140,0.3460],
 [0.4830,0.3120],
 [0.4780,0.4370],
 [0.5250,0.3690],
 [0.7510,0.4890],
 [0.5320,0.4720],
 [0.4730,0.3760],
 [0.7250,0.4450],
 [0.4460,0.4590]
]
```

- 处理过程：

  - 计算每一个测试数据集的元素与所有训练集的元素距离（采用欧式距离）。
  - 排序，并取前K个训练集的元素。
  - 计算K个训练集中出现频率最大的。出现频度最大的lable即为该测试元素的lable。

- 代码实现：

  ```python
  #encoding:utf-8
  from numpy import*
  import operator
  def myED(testdata,traindata):
      """ 计算欧式距离，要求测试样本和训练样本以array([ [],[],...[] ])的形式组织，
      每行表示一个样本，一列表示一个属性"""
      size_train=traindata.shape[0] # 训练样本量大小
      size_test=testdata.shape[0] # 测试样本大小
      XX=traindata**2
      sumXX=XX.sum(axis=1) # 行平方和
      YY=testdata**2
      sumYY=YY.sum(axis=1) # 行平方和
      Xpw2_plus_Ypw2=tile(mat(sumXX).T,[1,size_test])+\
      tile(mat(sumYY),[size_train,1])
      EDsq=Xpw2_plus_Ypw2-2*(mat(traindata)*mat(testdata).T) # 欧式距离平方
      distances=array(EDsq)**0.5 #欧式距离
      return distances

  def mykNN(testdata,traindata,labels,k):
      """ kNN算法主函数，labels组织成列表形式 """
      size_test=testdata.shape[0]
      D=myED(testdata,traindata)
      Dsortindex=D.argsort(axis=0) # 距离排序，提取序号
      nearest_k=Dsortindex[0:k,:] # 提取最近k个距离的样本序号
      label_nearest_k=array(labels)[nearest_k] # 提取最近k个距离样本的标签
      label_test=[]
      if k==1:
          label_test=label_nearest_k
      else:
          for smp in range(size_test):
              classcount={}
              labelset=set(label_nearest_k[:,smp]) # k个近邻样本的标签集合
              for label in labelset:
                  classcount[label]=list(label_nearest_k[:,smp]).count(label)
                  # 遍历k个近邻样本的标签，并计数，并以字典保存标签和计数结果
              sortedclasscount=sorted(classcount.items(),\
              key=operator.itemgetter(1),reverse=True) # 按照计数结果排序
              label_test.append(sortedclasscount[0][0]) # 提取出现最多的标签
      return label_test,D
  # 以下示例数据摘自周志华《机器学习》P202表9.1
  labels=[1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0]
  traindata=array([[0.6970,0.4600],[0.7740,0.3760],[0.6340,0.2640],\
  [0.6080,0.3180],[0.5560,0.2150],[0.4030,0.2370],[0.4810,0.1490],\
  [0.4370,0.2110],[0.6660,0.0910],[0.2430,0.2670],[0.2450,0.0570],\
  [0.3430,0.0990],[0.6390,0.1610],[0.6570,0.1980],[0.3600,0.3700],\
  [0.5930,0.0420],[0.7190,0.1030]])
  testdata=array([[0.3590,0.1880],[0.3390,0.2410],[0.2820,0.2570],\
  [0.7480,0.2320],[0.7140,0.3460],[0.4830,0.3120],[0.4780,0.4370],\
  [0.5250,0.3690],[0.7510,0.4890],[0.5320,0.4720],[0.4730,0.3760],\
  [0.7250,0.4450],[0.4460,0.4590]])
  k=5

  label_test,distances=mykNN(testdata,traindata,labels,k)
  print('\n')
  print(label_test)
  ```

- 结果输出：

  ```
  [1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
  ```

  ​